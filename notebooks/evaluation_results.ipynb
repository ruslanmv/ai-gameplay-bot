{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Paths to test results\n",
    "NN_RESULTS_PATH = \"results/nn_predictions.csv\"\n",
    "TRANSFORMER_RESULTS_PATH = \"results/transformer_predictions.csv\"\n",
    "\n",
    "# Load prediction results\n",
    "nn_results = pd.read_csv(NN_RESULTS_PATH)\n",
    "transformer_results = pd.read_csv(TRANSFORMER_RESULTS_PATH)\n",
    "\n",
    "# Preview the datasets\n",
    "print(\"Neural Network Results:\")\n",
    "print(nn_results.head())\n",
    "\n",
    "print(\"\\nTransformer Results:\")\n",
    "print(transformer_results.head())\n",
    "\n",
    "# Define a function to calculate evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1-score.\n",
    "    Args:\n",
    "        y_true (list): True labels.\n",
    "        y_pred (list): Predicted labels.\n",
    "    Returns:\n",
    "        dict: Dictionary of metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Extract true and predicted labels\n",
    "nn_y_true = nn_results[\"true_label\"]\n",
    "nn_y_pred = nn_results[\"predicted_label\"]\n",
    "\n",
    "transformer_y_true = transformer_results[\"true_label\"]\n",
    "transformer_y_pred = transformer_results[\"predicted_label\"]\n",
    "\n",
    "# Calculate metrics for both models\n",
    "nn_metrics = calculate_metrics(nn_y_true, nn_y_pred)\n",
    "transformer_metrics = calculate_metrics(transformer_y_true, transformer_y_pred)\n",
    "\n",
    "print(\"Neural Network Metrics:\")\n",
    "print(nn_metrics)\n",
    "\n",
    "print(\"\\nTransformer Metrics:\")\n",
    "print(transformer_metrics)\n",
    "\n",
    "# Define a function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix.\n",
    "    Args:\n",
    "        y_true (list): True labels.\n",
    "        y_pred (list): Predicted labels.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices for both models\n",
    "print(\"Neural Network Confusion Matrix:\")\n",
    "plot_confusion_matrix(nn_y_true, nn_y_pred, \"Confusion Matrix (Neural Network)\")\n",
    "\n",
    "print(\"Transformer Confusion Matrix:\")\n",
    "plot_confusion_matrix(transformer_y_true, transformer_y_pred, \"Confusion Matrix (Transformer)\")\n",
    "\n",
    "# Combine metrics for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": list(nn_metrics.keys()),\n",
    "    \"Neural Network\": list(nn_metrics.values()),\n",
    "    \"Transformer\": list(transformer_metrics.values())\n",
    "})\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"value\", hue=\"Model\", data=pd.melt(comparison_df, id_vars=\"Metric\", var_name=\"Model\", value_name=\"value\"))\n",
    "plt.title(\"Comparison of Model Performance Metrics\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "# Class-level precision, recall, and F1-score\n",
    "def calculate_class_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate class-level precision, recall, and F1-score.\n",
    "    Args:\n",
    "        y_true (list): True labels.\n",
    "        y_pred (list): Predicted labels.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of class-level metrics.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=None, labels=np.unique(y_true))\n",
    "    recall = recall_score(y_true, y_pred, average=None, labels=np.unique(y_true))\n",
    "    f1 = f1_score(y_true, y_pred, average=None, labels=np.unique(y_true))\n",
    "    metrics = pd.DataFrame({\n",
    "        \"Class\": np.unique(y_true),\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "nn_class_metrics = calculate_class_metrics(nn_y_true, nn_y_pred)\n",
    "transformer_class_metrics = calculate_class_metrics(transformer_y_true, transformer_y_pred)\n",
    "\n",
    "# Plot class-level metrics\n",
    "def plot_class_metrics(metrics, title):\n",
    "    \"\"\"\n",
    "    Plot class-level metrics as a grouped bar chart.\n",
    "    Args:\n",
    "        metrics (pd.DataFrame): DataFrame of class-level metrics.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    melted = metrics.melt(id_vars=\"Class\", var_name=\"Metric\", value_name=\"Value\")\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x=\"Class\", y=\"Value\", hue=\"Metric\", data=melted)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Class-Level Metrics (Neural Network):\")\n",
    "plot_class_metrics(nn_class_metrics, \"Class-Level Metrics (Neural Network)\")\n",
    "\n",
    "print(\"Class-Level Metrics (Transformer):\")\n",
    "plot_class_metrics(transformer_class_metrics, \"Class-Level Metrics (Transformer)\")\n",
    "\n",
    "# Highlight strengths and weaknesses\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"Neural Network:\")\n",
    "print(nn_metrics)\n",
    "\n",
    "print(\"\\nTransformer:\")\n",
    "print(transformer_metrics)\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "if nn_metrics[\"Accuracy\"] > transformer_metrics[\"Accuracy\"]:\n",
    "    print(\"- Neural Network performs better in overall accuracy.\")\n",
    "else:\n",
    "    print(\"- Transformer model outperforms Neural Network in overall accuracy.\")\n",
    "\n",
    "if nn_class_metrics[\"F1 Score\"].mean() > transformer_class_metrics[\"F1 Score\"].mean():\n",
    "    print(\"- Neural Network is more consistent across classes.\")\n",
    "else:\n",
    "    print(\"- Transformer model handles class imbalances more effectively.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
